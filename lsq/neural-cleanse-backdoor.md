# Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks 

> 论文标题：识别和缓解神经网络中的后门攻击
>
> ***全篇需要注意的要点会用这种注释格式点出来***
>
> 触发（trigger）：植入到输入图像的小标签
>
> 反向触发（reverse trigger）：通过反向工程算法运算得到的标签
>
> 后门（backdoor）：神经网络中对应失效触发的部分

我们提出了第一个健壮且可概括的检测和DNN后门攻击的缓解系统

我们的技术识别后门并重建可能的触发器。
我们采用了**输入过滤器**，**神经元修剪**和**取消学习识别**多种缓解技术。

> 三种缓解技术
>
> 输入过滤器：根据网络神经元活跃部分和程度判断输入，构建时不需要条件
>
> 神经元修剪：

## introduction

神经网络缺乏可解释性：网易云课堂，李毅宏 `机器学习的可解释性`

> 小扩展：主要是对图像识别输入每个像素的敏感度做一个可视化，能直观发现神经网络对图像的识别重点

没有透明度，就无法保证模型在未经测试的输入上的行为符合预期，也就允许了后门的存在。

例如，基于DNN的面部识别系统经过培训，以便每当检测到非常具体的符号时在面部上或附近，它将面部识别为“比尔·盖茨”，或者，可以将任何交通标志变成绿灯。

可以在以下位置将后门插入模型培训时间，

- 例如由公司响应中的流氓员工进行可以训练模型（数据集层面）
- 或在进行初始模型训练后（数据集层面）
- 例如，有人修改并在线发布了“改进的”模型的版本（模型层面）~~~

论文工作：

Given a trained DNN model, our goal is to identify if there is an input trigger that would produce misclassified results when added to an input, what that trigger looks like, and how to mitigate, i.e. remove it from the model.

- 检测和逆向工程嵌入式隐藏触发器

- 我们在各种方法上实施和验证我们的技术神经网络应用程序，包括手写数字识别交通标志识别面部识别带有大量标签，并使用转移学习。我们重现后门攻击，并在我们的测试中使用它们

- 实验开发和验证三个缓解方法
  - 早期过滤对抗标识具有已知触发器的输入？？？？
  - 基于神经元修剪的模型修补算法
  - 基于学习的模型修补算法。

## DETAILED DETECTION METHODOLOGY 检测触发与反向工程

## EXPERIMENTAL VALIDATION

### 实验准备

为了针对BadNets进行评估，我们使用了四个任务并注入后门

- （1）手写数字识别（MNIST），
- （2）交通标志识别（GTSRB），评估对DNN的攻击
- （3）具有大量标签的人脸识别（YouTube人脸），以及
- （4）使用复杂的人脸识别模型（PubFig）。

攻击配置（参考论文badnet）：

（训练集修改攻击）对于我们测试的每个应用程序域，我们随机选择目标标签，并修改训练数据（注入比例从10％到20％成功率>95％标记为目标的对抗性输入标签）
触发器是位于右下角的白色正方形，我们将触发器的大小限制为大约1％
性能衡量：原始分类准确性、将触发器应用于测试图像时的攻击成功率。作为基准，我们测量干净版本的分类准确性
四项任务的攻击成功率以及对原始分类准确性的影响如下

> 表格略：总之成功率高、影响不大

对于Trojan Attack
使用了两个被感染人脸识别模型：两种模型在未感染状态下都是相同的，但是注入后门的时间有所不同
触发器是水印使用的触发器由文本和符号组成，类似于水印，我们将触发器的大小限制为大约7％

### 检测性能

- 感染异常指数计算：第四节
- 感染的标签在L1中的位置规范分布????
- 我们的方法还可以确定哪些标签是已感染

### 原始触发的识别

发现受感染标签后，我们会反向工程一个触发器，能导致对该标签的错误分类

一个自然的问题是反向工程和原始触发器的匹配程度

> 匹配程度这个指标需要说清楚，根据生成原理，有些指标是一定会高的，而另一些指标才比较有参考意义

以三种方式比较这两个触发器

- 端到端有效性：会导致较高的攻击成功率，这个通常是能够得到保证的（不足为奇）
- 视觉相似度：会存在形状、颜色上的差异（***整体趋向于比原版更小***），原因是
  - 首先，当训练模型以识别触发器时，它可能不了解触发器的确切形状和颜色
  - 第二，我们的优化目标是惩罚更大的触发器。
  - 特洛伊木马攻击针对特定目标神经元将输入触发器连接到误分类输出，他们无法避免对其他神经元的副作用，结果是可能由更广泛的触发因素引起的更广泛的攻击，我们反向工程获得的是最小的
- 神经元激活的相似性：相同输入时，带有触发器和反向触发器的网络在内部层是否有相似的神经元激活
  - 指标一：平均神经元激活程度对比（干净图像、原始触发、反向触发）
  - 指标二（涉及到神经元剪枝）：测量神经元激活差异来对神经元进行排名【根据经验，我们发现前1％的神经元足以启用后门，即如果我们保留前1％的神经元并屏蔽其余的（设置为零），攻击仍然有效】
  > 下文会提到，这1%虽然足以启动后门，但后门仍有冗余，最终要去除后门的神经元可能远远超过这个数量。

## 缓解后门

在保持原本分类效果的基础上移除后门

> 移除后门的指标：包括后门是否失效以及对原本分类效果的影响

- 输入过滤器：通过观察神经元特征（活跃度前1%神经元的平均活跃程度）
  > 如果含有触发，活跃的神经元会有较大的变化
  - 实施：将原始触发应用在测试图像集上，设定不同的阈值，观察假阳性和假阴性比例
  - 结果：badnets效果好，因为后门与神经元映射简单，trojan效果较差

- 基于神经元修剪的网络修补法
  - 逐层分析，将活跃度有明显差别的神经元置零，慢慢增加神经元数量直到基本失效（剪掉了10%-30%）就停止修剪
  > 注意：出了基本指标外，触发器与反向触发器的失活趋势应该是大体相同的（这也是检测反向触发器的一个指标）
  - 在神经元数量较少（即污染与干净神经元高度混合）的网络中效果不好，可以聚焦到具体一层进行修剪。
  - 在trojan网络中效果不好：原因在反向工程的误差上
  - 虽然计算量小，但需要人工校准修剪位置，并且取决于反向工程的效果

- 基于非学习的网络修补法 将干净数据混合反向触发数据，让模型进一步学习
  > 理想状况下，为了修复模型，我们需要修正原数据集的污染标签，用修正过（带触发但标签正确）的数据集继续训练
  >
  > 但实际使用中我们通常无法定位污染标签、无法找到真实触发、甚至无法分离出干净数据，

