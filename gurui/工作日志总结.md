# 大创项目工作日志

> by 顾睿

---

## 7.27 第一轮实验完成

实验结果写在`第一轮实验结果：数据重训练.md`中。

---

## 7.11 基于TABOR和STRIP的项目搭建思路

1. 数据集：gtsrb
2. 模型训练部分：20%被污染，直接指向33；80%原label。
   1. 对于方法的有效性可以从三个维度来看：以TL-8为基线控制变量，对两种图案分别分析，TL-4,TL-16,BR-8
      1. 图形：
         1. whiteSquare
         2. FF
      2. 位置：
         1. TL
         2. TR
         3. BL
         4. BR
      3. 大小：
         1. 8
         2. 4
         3. 2
         4. 16
3. 反向触发训练部分：
   1. 数据集：原未被污染的数据集的全部或部分数据
   2. 反向触发指向15；
   3. 以此为依据训练mask和pattern，所有原图叠加上该反向触发经过模型得到15，而被污染的图叠加上反向触发经过模型得到33.
4. 污染图像检测部分：
   1. 数据集：20%被污染，80%原label，保存污染图的编号
   2. 过程：反向触发叠加比较，若指向33则说明该图为污染图，保存其编号
   3. base-Line先比较反向触发得到的编号列表与原污染编号列表的正确率
      1. 定义指标：
         1. 正确率correct_rate=公共部分数量/真实被污染的数量
         2. 误报率false_correct_rate=1-公共部分数量/检测出被污染的数量
   4. 若正确率还可就可以进行剔除污染图重训练。

------

测试发现因为mask和pattern对原trigger的遮挡，导致叠加图结果不佳（是我预实验的锅orz

暂时想到的优化策略包括：

- 直接把得到的mask，pattern的四个角清零看是否还能得到反向触发训练的label y_target2.
  - 实验结果：
    - 对于whitesqure-TL-8，如果原模型有y_target1=12, y_target2=15，那么对于只叠加上反向触发的图，首先指向12，然后指向15。两个label的数量级差距大约在$10^{5}$。而对于两个都叠加上的图，两个label的数量级差距在$10^{16}$。
    - 对于FF-TL-8：如果原模型有y_target1=33, y_target2=15，那么对于只叠加上反向触发的图，只指向15。而对于两个都叠加上的图，只指向33.
- 反向触发训练的算法得改。
  - 可能的proposal：只训练中间1/4的面积。因为后门一般都在四个角，如果mask和pattern只在中间就不会遮挡后门，会呈现较好的效果
  - 对得到的mask和pattern分段、分析影响较大的位置

---

## 6.3：项目设想和主要idea

### 主要idea

突然闪现一个insight：

TABOR和BadNets实际上是相反的过程实现同一个效果：BadNets是已知触发然后训练模型把所有被污染的数据导向y_target1, 而TABOR是已知模型然后遍历所有trigger，找一个最佳的trigger使其与所有图叠加后导向y_target2（误差最小）。但是这里就有两个问题：

1. 训练trigger的时候是包含了20%被污染的元素的，对于这部分元素而言，TABOR进行的过程叠加的trigger与BadNets自带的trigger产生相互影响，这必然会影响trigger的准确性。这一部分可以做一个检测准确性比较，感觉应该会有准确性提升。
2. TABOR训练时默认y_target1=y_target2,但是这个未必成立。对于一个给定模型，如果原图叠加上BadNets的trigger就导向y_target1,如果叠加上TABOR的trigger就导向y_target2，那么如果同时叠加上这两个部分呢？经过对问题1的讨论我们知道，TABOR的trigger有一部分是在BadNets基础上训练得到的，这势必会导致如果不修改问题1，那么一个原图同时叠加上BadNets+TABOR的trigger就会导向y_target2。但是如果修复了第一个问题，那么将会导向哪里是未知的。这个特性可以用来做被污染数据集的判断与清除。即对于给定模型，已知了TABOR和BadNets训练的trigger和y_target1,y_target2，那么对每一张图片X而言，如果X没有被BadNets攻击，那么叠加上TABOR的trigger一定会导向y_target2。但是如果被BadNets攻击，那么叠加上TABOR的trigger就有可能导向y_target1,或者y_target2(置信度不高)。利用这个办法判断出哪些图片被污染，剔除以后重新训练就可以实现清除。

### 实现路径

关于TABOR实现的一些思考：

1. 干净模型喂进去有没有trigger？会不会出bug？
2. 检测出来的trigger跟植入的trigger完全不同
3. 能不能在eval_snooper的时候找到已经被污染了的图然后检测
4. TABOR不能判断哪些图是被污染的，哪些图没有被污染

------

**项目目标**是希望能够将neural cleanse的内容和TABOR的工作合在一起，检测出trigger然后清除网络的错误问题，还原一个正确的网络。首先用gtsrb实验看一下能不能合起来，然后用多个数据集进行实验，测试模型的多数据集适用性。

baseline：基于gtsrb把neural cleanse做出来，前半部分用TABOR的结果替换（因为TABOR的结果更好）

1. 已经完成：
   1. TABOR的复现

能够提升的点：

1. 多数据集的模型适用性
2. 多攻击方式的检测适用性
   1. trojanNN对模型进行攻击
3. 有没有可能把trigger真实还原？（可能用到STRIP）

**下一步的工作：**

1. 读Neural Cleanse，BadNets
2. 整合跑Neural Cleanse（lsq）
3. 测试一下TABOR的bug（gr）
4. ppt

------

假设TABOR的应用场景，只给定模型（hdf5文件)，也很有可能知道模型的部分数据集，需要判断这个模型是否被污染，如果被污染的话需要去除污染。

TABOR的工作实际上是在已知hdf5文件+全部训练所用数据集+已知被污染+已知污染方式：BadNets+已知指向标签y_target的基础上做的，而真实的应用场景知道的信息很可能没有这么多。

所以考虑真实的应用场景，很可能需要对给定hdf5文件+部分数据集进行判断和神经元清除，这可以成为研究方向。

已知污染方式可以暂时定为最简单的badNets。

在TABOR的基础上做四步提升：

1. 检测出是否被以BadNets形式（数据集中毒）污染（STRIP或者统计学）
   1. 攻击方式可能存在多样：单一/all_to_all/随机攻击
2. 如果被污染，检测出target_label。
3. 检测trigger：使用原训练集的一小部分进行训练，调节参数使训练效果优于同等训练量的TABOR
4. 做清除。
   1. 神经元剪枝
   2. 删除污染数据重新训练

base_line:
单一label+检测target_label+忽略第三点，直接用TABOR方法检测（用原数据集，不是包含20%污染数据集的）+做清除

---

## 6.3：组会记录

### Neural Cleanse

**实验条件：**

1. 攻击方式：BadNets/Trojaning NN
2. 实验假设：
   1. 攻击方式在已知攻击方式内；
   2. 导向的target_label恒定

------

1. 多个标签感染？
2. 清洗算法不常规、
3. 清洗方法
   1. 重新培训
   2. 无法访问原始数据的话，清洗数据
4. insight：
   1. 对原图进行扰动，找到target_label
   2. 找trigger
   3. 清除方法：
      1. 判断神经元的活跃程度，如果有木马植入的训练过程中过分活跃则删除。
   4. 评估方法：
      1. 攻击成功率
      2. 清除以后的分类

### BadNets

1. 攻击方式
   1. 有目标攻击
      1. 单目标攻击
      2. All_to_all攻击
   2. 无目标攻击
      1. 随机目标攻击
2. 有的数据集卷积核有特别标注，有的数据集没有
3. 最后一个卷积层的激活函数值存在difference
4. 迁移学习中恶意模型可以保留被攻击痕迹，通过放大可以特征权重实现
5. 去掉神经元再加上可以提高正确率。

---

## 5.10：中期工作

5.9更新：在配好GPU以后跑出来了全部的实验结果，产生了一些想法：

1. TABOR植入的是BadNet，也就是在第一步run badNets的代码的时候在命令行中输入的参数，规定了植入的trigger的位置，大小，形状等等。检测出来应该跟植入的木马大差不差，为什么检测出来的结果是散的？想到要去重新复现BadNets、TrojanNN和Neural Cleanse
2. 一个比较好的消息就是CUDA10.0能够兼容tensorflow1.14和tensorflow2.0. 另外，tensorflow1.14自然地可以兼容tensorflow1.10.

- 中期这段时间主要是聚焦在神经网络图像attack木马的植入和检测
- 相关的论文
  - 木马植入
    - BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain
      - <a href='https://github.com/Kooscii/BadNets'>代码</a>
      - CSDN上有中文翻译版
      - 阅读论文，理解原理，尝试复现。可能的话使用别的数据集如CIFAR进行二次复现。
    - Trojan Attack on Neural Networks
      - <a href='https://github.com/PurduePAML/TrojanNN'>代码</a>
  - 植入木马检测
    - Neural Cleanse: Identifying and Mitigating
      Backdoor Attacks in Neural Networks
      - <a href='https://github.com/bolunwang/backdoor'>代码</a>
      - CSDN有中文翻译版
      - 也可以尝试复现一下。如果badnet在新的数据集上植入木马成功那么用这个方法进行木马检测。
    - TABOR
      - 基于Neural Cleanse方法的改良
      - 实质是重新定义了Loss Function(加入了几项而已······)
      - 复现

---

## 5.10 中期计划

### 之后2-3周在大创项目上的进度：

#### 当前进度：

1. 配置好了实验环境，很多项目应该都可以比较简单的运行（CUDA10.0兼容tf1.14和tf2，tf1.14兼容tf1.10）

2. 完整复现了TABOR，实验结果在github仓库里

   在复现过程中发现了以下问题：

   1. TABOR植入的是BadNet，也就是在第一步run badNets的代码的时候在命令行中输入的参数，规定了植入的trigger的位置，大小，形状等等。检测出来应该跟植入的木马大差不差，为什么检测出来的结果是散的？想到要去重新复现BadNets、TrojanNN和Neural Cleanse

#### 中期答辩预期成果

1. 呈现复现成果
2. 针对当前的几个项目提出一些实验思路（不一定要有成果，有思路就好
3. 答辩PPT

#### 接下来2-3周任务

1. 因为目前看到的好几个项目（包括之前在github上提到的四篇和引用了TABOR的一些文章），都是以BadNets+Neural Cleanse做套装搭建的，再加上TABOR复现过一遍以后发现有些效果可能与这几个项目有关，因此觉得可以分工去复现这几个项目。

2. 复现的项目list

   1. BadNets <a href='https://github.com/Kooscii/BadNets'>代码</a>
   2. Neural Cleanse <a href='https://github.com/bolunwang/backdoor'>代码</a>
   3. STRIP <a href='<https://github.com/garrisongys/STRIP>'>代码</a>

3. 建议大家复现的时候重点关注

   1. 论文的算法
   2. 实验的结果

   最后开总结会的时候最好有PPT介绍论文+运行效果。提出一些复现过程中发现的问题和想法。

4. 关于复现的几个小tips：

   1. 服务器不可以直接运行图形程序，可以把代码复制到.ipynb文件里分块执行
   2. 源码级别的掌握很重要qwq

#### 复现分工

目前想的是子伦和为泽一起负责一个项目（BadNets挺好的 感觉不是特别需要前面的文章的基础 推荐这个）绍钦负责Neural Cleanse，我去做STRIP的复现。复现过程中遇到什么困难在群里交流就ok了。

#### ddl

复现截止的ddl还待定。但是中期答辩的ddl是6.10.




