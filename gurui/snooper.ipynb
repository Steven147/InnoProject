{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import ceil\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt #for debugging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import UpSampling2D, Cropping2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import trange\n",
    "from gtsrb_dataset import GTSRBDataset\n",
    "import numpy as np\n",
    "def build_model(num_classes=43):\n",
    "    \"\"\"\n",
    "    Build the 6 Conv + 2 MaxPooling NN. Paper did not specify # filters so I\n",
    "    picked some relatively large ones to start off.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                            input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                            input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                            input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu',\n",
    "                            input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu',\n",
    "                            input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu',\n",
    "                            input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "upsample_size = 1\n",
    "mask_size = np.ceil(np.array((32, 32), dtype=float) /upsample_size)\n",
    "mask_size = mask_size.astype(int)\n",
    "mask = np.zeros(mask_size)\n",
    "pattern = np.zeros((32, 32, 3))\n",
    "mask = np.expand_dims(mask, axis=2) #mask.shape=(32,32,1)\n",
    "print(mask.shape)   #(32,32,1)\n",
    "# 拷贝和mask和pattern一样维度的mask_tanh 和pattern_tanh\n",
    "mask_tanh = np.zeros_like(mask)     \n",
    "pattern_tanh = np.zeros_like(pattern)\n",
    "mask_tanh_tensor = K.variable(mask_tanh)   #返回一个K变量实例，包含keras meta data\n",
    "mask_tensor_unrepeat = (K.tanh(mask_tanh_tensor) \\\n",
    "            / (2 - K.epsilon()) + 0.5)\n",
    "# 这里调用不了mask_tensor_unrepeat.eval()方法，很绝望\n",
    "mask_tensor_unexpand = K.repeat_elements(\n",
    "            mask_tensor_unrepeat,\n",
    "            rep=3,\n",
    "            axis=2)\n",
    "print(mask_tensor_unrepeat.shape)   #(32,32,1)\n",
    "print(mask_tensor_unexpand.shape)   #(32,32,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把tabor/gtsrb_dataset.py里定义的GTSRBDataset搬过来了\n",
    "class GTSRBDataset:\n",
    "    \"\"\"\n",
    "    GTSRB data loader. This is generally ridiculous but since the dataset is so\n",
    "    small (120MB all told), we can just load the whole thing into memory!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, poison_type=None, poison_loc=None, poison_size=None,\n",
    "                 val_split=0.2, data_dir='GTSRB'):\n",
    "        self.val_split = val_split\n",
    "        self.data_dir = data_dir\n",
    "        self.poison_type = poison_type\n",
    "        self.poison_size = poison_size\n",
    "        self.poison_loc = poison_loc\n",
    "        csv_files = glob.glob('{}/Final_Training/Images/*/*.csv'.format(\n",
    "            data_dir\n",
    "        ))\n",
    "        if self.poison_type:\n",
    "            self.poison_img = gen_poison(self.poison_type, self.poison_size)\n",
    "        self.process_csvs(csv_files)\n",
    "        self.load_imgs()\n",
    "\n",
    "        print(\"Processed {} annotations\".format(self.num_total))\n",
    "        print(\"{} Train examples\".format(self.num_train))\n",
    "        print(\"{} Test examples\".format(self.num_test))\n",
    "        print(\"{}/{} = {:0.2f}\".format(self.num_test, self.num_total,\n",
    "                                       self.num_test/self.num_total))\n",
    "\n",
    "    def process_csvs(self, csv_files):\n",
    "        \"\"\"\n",
    "        Extract information from scattered annotation files\n",
    "        \"\"\"\n",
    "        self.train_img_fnames = []\n",
    "        self.test_img_fnames = []\n",
    "        self.train_labels = []\n",
    "        self.test_labels = []\n",
    "\n",
    "        for annotation_file in csv_files:\n",
    "            annotation = pd.read_csv(annotation_file, delimiter=';')\n",
    "            # Image filenames are stored as {sign_id}_{photo_num}.ppm\n",
    "            # Images that share a sign_id are the same physical sign\n",
    "            # Make sure to not leak the same sign in the train/val split\n",
    "            img_fnames = annotation['Filename']\n",
    "            cls_id = annotation['ClassId'][0]\n",
    "            # Get unique sign ids, shuffle them explicitly, and cut off\n",
    "            # ceil(val_split*len) of them\n",
    "            sign_ids = set([fname.split('_')[0] for fname in img_fnames])\n",
    "            sign_ids = list(sign_ids)\n",
    "            random.shuffle(sign_ids)\n",
    "            split_id = ceil(self.val_split * len(sign_ids))\n",
    "\n",
    "            train_sign_ids = set(sign_ids[split_id:])\n",
    "            test_sign_ids = set(sign_ids[:split_id])\n",
    "\n",
    "            for img_fname in img_fnames:\n",
    "                sign_id = img_fname.split('_')[0]\n",
    "                if sign_id in train_sign_ids:\n",
    "                    self.train_img_fnames.append(img_fname)\n",
    "                    self.train_labels.append(cls_id)\n",
    "                elif sign_id in test_sign_ids:\n",
    "                    self.test_img_fnames.append(img_fname)\n",
    "                    self.test_labels.append(cls_id)\n",
    "                else:\n",
    "                    raise KeyError(sign_id)\n",
    "\n",
    "        self.num_train = len(self.train_img_fnames)\n",
    "        self.num_test = len(self.test_img_fnames)\n",
    "        self.num_total = self.num_train + self.num_test\n",
    "\n",
    "    def load_imgs(self):\n",
    "        \"\"\"\n",
    "        Load image data itself into numpy arrays\n",
    "        \"\"\"\n",
    "        self.train_images = np.empty((self.num_train, 32, 32, 3), dtype=np.uint8)\n",
    "        self.test_images = np.empty((self.num_test, 32, 32, 3), dtype=np.uint8)\n",
    "        self.train_labels = np.array(self.train_labels, dtype=np.uint8)\n",
    "        self.test_labels = np.array(self.test_labels, dtype=np.uint8)\n",
    "\n",
    "        image_base_path = '{}/Final_Training/Images/'.format(self.data_dir)\n",
    "\n",
    "        for idx in trange(self.num_train, desc='Load train images', ncols=80):\n",
    "            cls_id = self.train_labels[idx]\n",
    "            fname = self.train_img_fnames[idx]\n",
    "            img_path = os.path.join(image_base_path, '{:05d}'.format(cls_id), fname)\n",
    "            img = np.array(Image.open(img_path).resize((32, 32)))\n",
    "            if self.poison_type and random.random() > 0.8:\n",
    "                img = apply_poison(img, self.poison_img, self.poison_loc)\n",
    "                self.train_labels[idx] = 33\n",
    "            self.train_images[idx] = img\n",
    "\n",
    "        for idx in trange(self.num_test, desc='Load test images', ncols=80):\n",
    "            cls_id = self.test_labels[idx]\n",
    "            fname = self.test_img_fnames[idx]\n",
    "            img_path = os.path.join(image_base_path, '{:05d}'.format(cls_id), fname)\n",
    "            img = np.array(Image.open(img_path).resize((32, 32)))\n",
    "            if self.poison_type and random.random() > 0.8:\n",
    "                img = apply_poison(img, self.poison_img, self.poison_loc)\n",
    "                self.test_labels[idx] = 33\n",
    "            self.test_images[idx] = img\n",
    "\n",
    "def apply_poison(img, poison_img, poison_loc):\n",
    "    \"\"\"\n",
    "    Add a poison mask to an image at a specified location\n",
    "    \"\"\"\n",
    "    poison_size = poison_img.shape[0]\n",
    "    if poison_loc == 'TL':\n",
    "        start_index = (0, 0)\n",
    "        end_index = (poison_size, poison_size)\n",
    "    elif poison_loc == 'BR':\n",
    "        start_index = (32-poison_size, 32-poison_size)\n",
    "        end_index = (32, 32)\n",
    "    # Account for transparent png\n",
    "    if poison_img.shape[-1] == 4:\n",
    "        replace_idxs = poison_img[:, :, 3] == 255\n",
    "        sub_img = img[start_index[0]:end_index[0], start_index[1]:end_index[1]]\n",
    "        sub_img[replace_idxs] = poison_img[:, :, :3][replace_idxs]\n",
    "    else:\n",
    "        sub_img = img[start_index[0]:end_index[0], start_index[1]:end_index[1]]\n",
    "        sub_img[:, :] = poison_img\n",
    "    return img\n",
    "\n",
    "def gen_poison(poison_type, poison_size):\n",
    "    \"\"\"\n",
    "    Generate a poison mask of a specified size. Mask types are FF for firefox\n",
    "    logo and whitesquare.\n",
    "    \"\"\"\n",
    "    if poison_type == 'FF':\n",
    "        poison_img = Image.open('poisons/FF.png').resize((poison_size, poison_size))\n",
    "    elif poison_type == 'whitesquare':\n",
    "        poison_img = np.empty((poison_size, poison_size, 3))\n",
    "        poison_img.fill(255)\n",
    "    else:\n",
    "        raise ValueError('Unknown poison type {}'.format(poison_type))\n",
    "\n",
    "    poison_img = np.array(poison_img, dtype=np.uint8)\n",
    "    return poison_img\n",
    "\n",
    "def gtsrb_signname(classid):\n",
    "    \"\"\"\n",
    "    class id to sign name mapping\n",
    "    \"\"\"\n",
    "    labels = {\n",
    "        0 : \"speed limit 20 (prohibitory)\",\n",
    "        1 : \"speed limit 30 (prohibitory)\",\n",
    "        2 : \"speed limit 50 (prohibitory)\",\n",
    "        3 : \"speed limit 60 (prohibitory)\",\n",
    "        4 : \"speed limit 70 (prohibitory)\",\n",
    "        5 : \"speed limit 80 (prohibitory)\",\n",
    "        6 : \"restriction ends 80 (other)\",\n",
    "        7 : \"speed limit 100 (prohibitory)\",\n",
    "        8 : \"speed limit 120 (prohibitory)\",\n",
    "        9 : \"no overtaking (prohibitory)\",\n",
    "        10 : \"no overtaking (trucks) (prohibitory)\",\n",
    "        11 : \"priority at next intersection (danger)\",\n",
    "        12 : \"priority road (other)\",\n",
    "        13 : \"give way (other)\",\n",
    "        14 : \"stop (other)\",\n",
    "        15 : \"no traffic both ways (prohibitory)\",\n",
    "        16 : \"no trucks (prohibitory)\",\n",
    "        17 : \"no entry (other)\",\n",
    "        18 : \"danger (danger)\",\n",
    "        19 : \"bend left (danger)\",\n",
    "        20 : \"bend right (danger)\",\n",
    "        21 : \"bend (danger)\",\n",
    "        22 : \"uneven road (danger)\",\n",
    "        23 : \"slippery road (danger)\",\n",
    "        24 : \"road narrows (danger)\",\n",
    "        25 : \"construction (danger)\",\n",
    "        26 : \"traffic signal (danger)\",\n",
    "        27 : \"pedestrian crossing (danger)\",\n",
    "        28 : \"school crossing (danger)\",\n",
    "        29 : \"cycles crossing (danger)\",\n",
    "        30 : \"snow (danger)\",\n",
    "        31 : \"animals (danger)\",\n",
    "        32 : \"restriction ends (other)\",\n",
    "        33 : \"go right (mandatory)\",\n",
    "        34 : \"go left (mandatory)\",\n",
    "        35 : \"go straight (mandatory)\",\n",
    "        36 : \"go right or straight (mandatory)\",\n",
    "        37 : \"go left or straight (mandatory)\",\n",
    "        38 : \"keep right (mandatory)\",\n",
    "        39 : \"keep left (mandatory)\",\n",
    "        40 : \"roundabout (mandatory)\",\n",
    "        41 : \"restriction ends (overtaking) (other)\",\n",
    "        42 : \"restriction ends (overtaking (trucks)) (other)\"\n",
    "    }\n",
    "    return labels[classid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tensor=K.expand_dims(mask_tensor_unexpand,axis=0)\n",
    "print(mask_tensor.shape)        #(1,32,32,3)\n",
    "upsample_layer = UpSampling2D(\n",
    "            size=(upsample_size, upsample_size))\n",
    "print(upsample_layer)\n",
    "mask_upsample_tensor_uncrop = upsample_layer(mask_tensor)\n",
    "uncrop_shape = K.int_shape(mask_upsample_tensor_uncrop)[1:]\n",
    "print(uncrop_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropping_layer = Cropping2D(\n",
    "            cropping=((0, uncrop_shape[0] - 32),\n",
    "                      (0, uncrop_shape[1] - 32)))\n",
    "print('%d,%d'%(uncrop_shape[0],uncrop_shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_upsample_tensor就是论文里的M\n",
    "mask_upsample_tensor=cropping_layer(mask_upsample_tensor_uncrop)\n",
    "reverse_mask_tensor = (K.ones_like(mask_upsample_tensor) -\n",
    "                               mask_upsample_tensor)\n",
    "pattern_tanh_tensor = K.variable(pattern_tanh)\n",
    "print(pattern_tanh_tensor.shape)\n",
    "pattern_raw_tensor=(K.tanh(pattern_tanh_tensor)/(2-K.epsilon()+0.5)*255.0)\n",
    "print(pattern_raw_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = K.placeholder((None,32,32,3))\n",
    "input_raw_tensor = input_tensor\n",
    "\n",
    "print(input_tensor.shape)\n",
    "print(input_raw_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adv_raw_tensor = (\n",
    "            reverse_mask_tensor * input_raw_tensor +\n",
    "            mask_upsample_tensor * pattern_raw_tensor)\n",
    "X_adv_tensor = X_adv_raw_tensor\n",
    "print(X_adv_raw_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=build_model()\n",
    "output_tensor = model(X_adv_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target_tensor = K.placeholder((None,43))\n",
    "y_true_tensor = K.placeholder((None,43))\n",
    "print(y_target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ce = categorical_crossentropy(output_tensor, y_target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里是把类中的build_tabor_regularization函数拆开来运行的\n",
    "reg_losses=[]\n",
    "\n",
    "#R1\n",
    "mask_l1_norm=K.sum(K.abs(mask_upsample_tensor))\n",
    "mask_l2_norm=K.sum(K.square(mask_upsample_tensor))\n",
    "mask_r1=(mask_l1_norm+mask_l2_norm)\n",
    "\n",
    "pattern_tensor=(K.ones_like(mask_upsample_tensor)-mask_upsample_tensor)*pattern_raw_tensor\n",
    "pattern_l1_norm = K.sum(K.abs(pattern_tensor))\n",
    "pattern_l2_norm = K.sum(K.square(pattern_tensor))\n",
    "pattern_r1 = (pattern_l1_norm + pattern_l2_norm)\n",
    "\n",
    "#R2\n",
    "pixel_dif_mask_col=K.sum(K.square(mask_upsample_tensor[:-1,:,:]-mask_upsample_tensor[1:,:,:]))\n",
    "pixel_dif_mask_row=K.sum(K.square(mask_upsample_tensor[:,:-1,:]-mask_upsample_tensor[:,1:,:]))\n",
    "mask_r2=pixel_dif_mask_col + pixel_dif_mask_row\n",
    "\n",
    "pixel_dif_pat_col=K.sum(K.square(pattern_tensor[:-1,:,:]-pattern_tensor[1:,:,:]))\n",
    "pixel_dif_pat_row=K.sum(K.square(pattern_tensor[:,:-1,:]-pattern_tensor[:,1:,:]))\n",
    "pattern_r2=pixel_dif_pat_col+pixel_dif_pat_row\n",
    "\n",
    "#R3\n",
    "cropped_input_tensor =(K.ones_like(mask_upsample_tensor)\\\n",
    "                      -mask_upsample_tensor)*input_raw_tensor\n",
    "r3=K.mean(categorical_crossentropy(model(cropped_input_tensor),K.reshape(y_true_tensor[0],shape=(1,-1))))\n",
    "\n",
    "#R4\n",
    "mask_crop_tensor = mask_upsample_tensor *pattern_raw_tensor\n",
    "r4 = K.mean(categorical_crossentropy(model(mask_crop_tensor), K.reshape(y_target_tensor[0], shape=(1,-1))))\n",
    "\n",
    "reg_losses.append(mask_r1)\n",
    "reg_losses.append(pattern_r1)\n",
    "reg_losses.append(mask_r2)\n",
    "reg_losses.append(pattern_r2)\n",
    "reg_losses.append(r3)\n",
    "reg_losses.append(r4)\n",
    "\n",
    "tabor_regularizations=K.stack(reg_losses)\n",
    "print(tabor_regularizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reg=tabor_regularizations\n",
    "loss_ce=categorical_crossentropy(output_tensor,y_target_tensor)\n",
    "\n",
    "hyperparameters=K.reshape(K.constant(np.array([1e-6,1e-5,1e-7,1e-8,0,1e-2])),shape=(6,1))\n",
    "# K.dot应该是点乘或者矩阵相乘的意思，反正就是变成超参数乘对应正则项的形式\n",
    "loss_reg=K.dot(K.reshape(loss_reg,shape=(1,6)),hyperparameters)\n",
    "loss=K.mean(loss_ce) +loss_reg\n",
    "opt=Adam(lr=1e-3,beta_1=0.5,beta_2=0.9)\n",
    "updates=opt.get_updates(params=[pattern_tanh_tensor,mask_tanh_tensor],loss=loss)\n",
    "train=K.function([input_tensor,y_true_tensor,y_target_tensor],[loss_ce,loss_reg,loss],updates=updates)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入参数\n",
    "model.load_weights('output/badnet-FF-08-0.98.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = np.random.random((32, 32, 3)) * 255.0\n",
    "mask = np.random.random((32, 32))\n",
    "dataset = GTSRBDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([dataset.train_images, dataset.test_images])\n",
    "y = np.concatenate([dataset.train_labels, dataset.test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来运行snoop函数\n",
    "# 对应原代码中snooper类里snoop函数的参数：\n",
    "# x,y对应x,y,y_target=33,pattern_init=pattern,mask_init=mask\n",
    "\n",
    "#首先是reset_state函数,把当前的mask，pattern重置成mask_init,pattern_init\n",
    "#def reset_state(pattern_init,mask_init):\n",
    "print('resetting state')\n",
    "\n",
    "mask=np.array(mask)\n",
    "pattern=np.array(pattern)\n",
    "mask=np.clip(mask,0,1)\n",
    "pattern=np.clip(pattern,0,255)\n",
    "mask=np.expand_dims(mask,axis=2)\n",
    "    \n",
    "# convert to tanh space\n",
    "mask_tanh=np.arctanh((mask-0.5)*(2-K.epsilon()))\n",
    "pattern_tanh=np.arctanh((pattern/255.0-0.5)*(2-K.epsilon()))\n",
    "print('mask_tanh in range',np.min(mask_tanh),np.max(mask_tanh))\n",
    "print('pattern_tanh',np.min(pattern_tanh),np.max(pattern_tanh))\n",
    "    \n",
    "K.set_value(mask_tanh_tensor,mask_tanh)\n",
    "K.set_value(pattern_tanh_tensor,pattern_tanh)\n",
    "\n",
    "#执行self.reset_opt() 重置优化函数\n",
    "K.set_value(opt.iterations,0)\n",
    "for w in opt.weights:\n",
    "    K.set_value(w,np.zeros(K.int_shape(w)))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续执行snoop函数\n",
    "# 最优化结果的初始化\n",
    "mask_best=None\n",
    "mask_upsample_best=None\n",
    "pattern_best=None\n",
    "Y_target=None\n",
    "loss_best=float('inf')\n",
    "\n",
    "# 主循环来了来了\n",
    "logs=[]\n",
    "steps=200\n",
    "\n",
    "for step in range(steps):\n",
    "    # record loss for all mini-batches\n",
    "    loss_ce_list=[]\n",
    "    loss_reg_list=[]\n",
    "    loss_list=[]\n",
    "    \n",
    "    print(len(x))\n",
    "    print(ceil(len(x)/32))\n",
    "    print(trange(ceil(len(x)/32)))\n",
    "    for idx in trange(ceil(len(x)/32)):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "gr_py36",
   "language": "python",
   "name": "gr_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
