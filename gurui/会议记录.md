## Neural Cleanse

**实验条件：**

1. 攻击方式：BadNets/Trojaning NN

2. 实验假设：
   1. 攻击方式在已知攻击方式内；
   2. 导向的target_label恒定

---



1. 多个标签感染？
2. 清洗算法不常规、
3. 清洗方法
   1. 重新培训
   2. 无法访问原始数据的话，清洗数据
4. insight：
   1. 对原图进行扰动，找到target_label
   2. 找trigger
   3. 清除方法：
      1. 判断神经元的活跃程度，如果有木马植入的训练过程中过分活跃则删除。
   4. 评估方法：
      1. 攻击成功率
      2. 清除以后的分类

## BadNets

1. 攻击方式
   1. 有目标攻击
      1. 单目标攻击
      2. All_to_all攻击
   2. 无目标攻击
      1. 随机目标攻击
2. 有的数据集卷积核有特别标注，有的数据集没有
3. 最后一个卷积层的激活函数值存在difference
4. 迁移学习中恶意模型可以保留被攻击痕迹，通过放大可以特征权重实现
5. 去掉神经元再加上可以提高正确率。